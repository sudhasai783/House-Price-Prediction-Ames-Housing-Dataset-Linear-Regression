# ðŸ¡ House Price Prediction â€“ Ames Housing Dataset  

## ðŸ“Œ Overview  
This project predicts house prices using the **Ames Housing dataset** (79 features, ~2,900 records).  
I explored data cleaning, feature engineering, visualization, and applied multiple machine learning models to understand what drives house prices.  

The project demonstrates an **end-to-end Data Science workflow**: from raw data to insights and predictive modeling.  

---

## ðŸŽ¯ Objectives  
- Clean and preprocess the dataset (handle nulls, outliers, categorical encoding).  
- Perform **exploratory data analysis (EDA)** to identify key factors affecting SalePrice.  
- Build regression models to predict **log-transformed SalePrice**.  
- Compare Linear Regression with Ridge, Lasso, Random Forest, and Gradient Boosting.  
- Evaluate performance using **RÂ², RMSE, and cross-validation**.  

---

## ðŸ›  Tools & Libraries  
- **Python** (pandas, numpy, matplotlib, seaborn, scikit-learn)  
- **Machine Learning Models:** Linear Regression, Ridge, Lasso, RandomForest, Gradient Boosting  
- **Visualization:** Matplotlib, Seaborn  
- **Other:** Cross-validation, GridSearchCV for hyperparameter tuning  

---

## ðŸ“Š Workflow  

### 1. Data Preprocessing  
- Dropped high-null columns (`PoolQC`, `MiscFeature`, `Alley`, `Fence`).  
- Imputed missing values (mean for numeric, mode for categorical).  
- Removed outliers (`GrLivArea > 4000` with low SalePrice).  
- Applied **log transformation** to SalePrice and skewed features.  
- One-hot encoded categorical variables.  

### 2. Exploratory Data Analysis (EDA)  
- Correlation heatmaps â†’ **OverallQual, GrLivArea, GarageCars, TotalBsmtSF** were top predictors.  
- Scatter plots & boxplots to visualize relationships.  
- Identified redundancy between features (e.g., `GarageCars` vs `GarageArea`).  

### 3. Modeling  
- **Baseline Linear Regression:** RÂ² = 0.90, RMSE â‰ˆ 0.127  
- **Ridge Regression:** RÂ² = 0.917, RMSE â‰ˆ 0.115 (Best performance)  
- **Lasso Regression:** RÂ² = 0.913, RMSE â‰ˆ 0.118  
- **Gradient Boosting:** RÂ² = 0.903, RMSE â‰ˆ 0.124  
- **RandomForest:** RÂ² = 0.876, RMSE â‰ˆ 0.140  

### 4. Evaluation  
- Used **cross-validation (5-fold)** to confirm robustness.  
- Visualized **Predicted vs Actual** and **residual plots** for diagnostics.  

---

## ðŸ“ˆ Results  
- **Ridge Regression** provided the best balance between accuracy and stability.  
- Cross-validation confirmed Ridge (RÂ² â‰ˆ 0.92, RMSE â‰ˆ 11.5%) was the most reliable model.  
- Top predictors: **OverallQual, GrLivArea, GarageCars, YearBuilt, TotalBsmtSF**.  

---

## ðŸš€ Key Learnings  
- Feature engineering (log transforms, encoding) can significantly improve model accuracy.  
- Regularization (Ridge/Lasso) helps combat multicollinearity in high-dimensional datasets.  
- Even simple models can outperform complex ones if the data is well-prepared.  


